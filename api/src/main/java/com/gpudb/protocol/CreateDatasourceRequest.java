/*
 *  This file was autogenerated by the Kinetica schema processor.
 *
 *  DO NOT EDIT DIRECTLY.
 */
package com.gpudb.protocol;

import java.util.LinkedHashMap;
import java.util.Map;
import org.apache.avro.Schema;
import org.apache.avro.SchemaBuilder;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.IndexedRecord;

/**
 * A set of parameters for {@link
 * com.gpudb.GPUdb#createDatasource(CreateDatasourceRequest)
 * GPUdb.createDatasource}.
 * <p>
 * Creates a <a href="../../../../../../concepts/data_sources/"
 * target="_top">data source</a>, which contains the location and connection
 * information for a data store that is external to the database.
 */
public class CreateDatasourceRequest implements IndexedRecord {
    private static final Schema schema$ = SchemaBuilder
            .record("CreateDatasourceRequest")
            .namespace("com.gpudb")
            .fields()
                .name("name").type().stringType().noDefault()
                .name("location").type().stringType().noDefault()
                .name("userName").type().stringType().noDefault()
                .name("password").type().stringType().noDefault()
                .name("options").type().map().values().stringType().noDefault()
            .endRecord();

    /**
     * This method supports the Avro framework and is not intended to be called
     * directly by the user.
     *
     * @return The schema for the class.
     */
    public static Schema getClassSchema() {
        return schema$;
    }

    /**
     * A set of string constants for the {@link CreateDatasourceRequest}
     * parameter {@link #getOptions() options}.
     * <p>
     * Optional parameters.
     */
    public static final class Options {
        /**
         * Bypass validation of connection to remote source.
         * Supported values:
         * <ul>
         *     <li>{@link Options#TRUE TRUE}
         *     <li>{@link Options#FALSE FALSE}
         * </ul>
         * The default value is {@link Options#FALSE FALSE}.
         */
        public static final String SKIP_VALIDATION = "skip_validation";

        public static final String TRUE = "true";
        public static final String FALSE = "false";

        /**
         * Timeout in seconds for connecting to this storage provider
         */
        public static final String CONNECTION_TIMEOUT = "connection_timeout";

        /**
         * Timeout in seconds for reading from this storage provider
         */
        public static final String WAIT_TIMEOUT = "wait_timeout";

        /**
         * Name of the Credential object to be used in data source
         */
        public static final String CREDENTIAL = "credential";

        /**
         * Name of the Amazon S3 bucket to use as the data source
         */
        public static final String S3_BUCKET_NAME = "s3_bucket_name";

        /**
         * Name of the Amazon S3 region where the given bucket is located
         */
        public static final String S3_REGION = "s3_region";

        /**
         * When true (default), the requests URI should be specified in
         * virtual-hosted-style format where the bucket name is part of the
         * domain name in the URL.
         * <p>
         * Otherwise set to false to use path-style URI for requests.
         * Supported values:
         * <ul>
         *     <li>{@link Options#TRUE TRUE}
         *     <li>{@link Options#FALSE FALSE}
         * </ul>
         * The default value is {@link Options#TRUE TRUE}.
         */
        public static final String S3_USE_VIRTUAL_ADDRESSING = "s3_use_virtual_addressing";

        /**
         * Amazon IAM Role ARN which has required S3 permissions that can be
         * assumed for the given S3 IAM user
         */
        public static final String S3_AWS_ROLE_ARN = "s3_aws_role_arn";

        /**
         * Customer encryption algorithm used encrypting data
         */
        public static final String S3_ENCRYPTION_CUSTOMER_ALGORITHM = "s3_encryption_customer_algorithm";

        /**
         * Customer encryption key to encrypt or decrypt data
         */
        public static final String S3_ENCRYPTION_CUSTOMER_KEY = "s3_encryption_customer_key";

        /**
         * Kerberos keytab file location for the given HDFS user.  This may be
         * a KIFS file.
         */
        public static final String HDFS_KERBEROS_KEYTAB = "hdfs_kerberos_keytab";

        /**
         * Delegation token for the given HDFS user
         */
        public static final String HDFS_DELEGATION_TOKEN = "hdfs_delegation_token";

        /**
         * Use kerberos authentication for the given HDFS cluster.
         * Supported values:
         * <ul>
         *     <li>{@link Options#TRUE TRUE}
         *     <li>{@link Options#FALSE FALSE}
         * </ul>
         * The default value is {@link Options#FALSE FALSE}.
         */
        public static final String HDFS_USE_KERBEROS = "hdfs_use_kerberos";

        /**
         * Name of the Azure storage account to use as the data source, this is
         * valid only if tenant_id is specified
         */
        public static final String AZURE_STORAGE_ACCOUNT_NAME = "azure_storage_account_name";

        /**
         * Name of the Azure storage container to use as the data source
         */
        public static final String AZURE_CONTAINER_NAME = "azure_container_name";

        /**
         * Active Directory tenant ID (or directory ID)
         */
        public static final String AZURE_TENANT_ID = "azure_tenant_id";

        /**
         * Shared access signature token for Azure storage account to use as
         * the data source
         */
        public static final String AZURE_SAS_TOKEN = "azure_sas_token";

        /**
         * Oauth token to access given storage container
         */
        public static final String AZURE_OAUTH_TOKEN = "azure_oauth_token";

        /**
         * Name of the Google Cloud Storage bucket to use as the data source
         */
        public static final String GCS_BUCKET_NAME = "gcs_bucket_name";

        /**
         * Name of the Google Cloud project to use as the data source
         */
        public static final String GCS_PROJECT_ID = "gcs_project_id";

        /**
         * Google Cloud service account keys to use for authenticating the data
         * source
         */
        public static final String GCS_SERVICE_ACCOUNT_KEYS = "gcs_service_account_keys";

        /**
         * To load from Azure/GCS/S3 as a stream continuously.
         * Supported values:
         * <ul>
         *     <li>{@link Options#TRUE TRUE}
         *     <li>{@link Options#FALSE FALSE}
         * </ul>
         * The default value is {@link Options#FALSE FALSE}.
         */
        public static final String IS_STREAM = "is_stream";

        /**
         * Name of the Kafka topic to use as the data source
         */
        public static final String KAFKA_TOPIC_NAME = "kafka_topic_name";

        /**
         * JDBC driver jar file location.  This may be a KIFS file.
         */
        public static final String JDBC_DRIVER_JAR_PATH = "jdbc_driver_jar_path";

        /**
         * Name of the JDBC driver class
         */
        public static final String JDBC_DRIVER_CLASS_NAME = "jdbc_driver_class_name";

        /**
         * Use anonymous connection to storage provider--DEPRECATED: this is
         * now the default.  Specify use_managed_credentials for non-anonymous
         * connection.
         * Supported values:
         * <ul>
         *     <li>{@link Options#TRUE TRUE}
         *     <li>{@link Options#FALSE FALSE}
         * </ul>
         * The default value is {@link Options#TRUE TRUE}.
         */
        public static final String ANONYMOUS = "anonymous";

        /**
         * When no credentials are supplied, we use anonymous access by
         * default.  If this is set, we will use cloud provider user settings.
         * Supported values:
         * <ul>
         *     <li>{@link Options#TRUE TRUE}
         *     <li>{@link Options#FALSE FALSE}
         * </ul>
         * The default value is {@link Options#FALSE FALSE}.
         */
        public static final String USE_MANAGED_CREDENTIALS = "use_managed_credentials";

        /**
         * Use https to connect to datasource if true, otherwise use http.
         * Supported values:
         * <ul>
         *     <li>{@link Options#TRUE TRUE}
         *     <li>{@link Options#FALSE FALSE}
         * </ul>
         * The default value is {@link Options#TRUE TRUE}.
         */
        public static final String USE_HTTPS = "use_https";

        /**
         * Location of Confluent Schema registry in
         * '[storage_path[:storage_port]]' format.
         */
        public static final String SCHEMA_REGISTRY_LOCATION = "schema_registry_location";

        /**
         * Confluent Schema registry Credential object name.
         */
        public static final String SCHEMA_REGISTRY_CREDENTIAL = "schema_registry_credential";

        /**
         * Confluent Schema registry port (optional).
         */
        public static final String SCHEMA_REGISTRY_PORT = "schema_registry_port";

        private Options() {  }
    }

    private String name;
    private String location;
    private String userName;
    private String password;
    private Map<String, String> options;

    /**
     * Constructs a CreateDatasourceRequest object with default parameters.
     */
    public CreateDatasourceRequest() {
        name = "";
        location = "";
        userName = "";
        password = "";
        options = new LinkedHashMap<>();
    }

    /**
     * Constructs a CreateDatasourceRequest object with the specified
     * parameters.
     *
     * @param name  Name of the data source to be created.
     * @param location  Location of the remote storage in
     *                  'storage_provider_type://[storage_path[:storage_port]]'
     *                  format.  Supported storage provider types are
     *                  'azure','gcs','hdfs','jdbc','kafka', 'confluent' and
     *                  's3'.
     * @param userName  Name of the remote system user; may be an empty string
     * @param password  Password for the remote system user; may be an empty
     *                  string
     * @param options  Optional parameters.
     *                 <ul>
     *                     <li>{@link Options#SKIP_VALIDATION SKIP_VALIDATION}:
     *                         Bypass validation of connection to remote
     *                         source.
     *                         Supported values:
     *                         <ul>
     *                             <li>{@link Options#TRUE TRUE}
     *                             <li>{@link Options#FALSE FALSE}
     *                         </ul>
     *                         The default value is {@link Options#FALSE
     *                         FALSE}.
     *                     <li>{@link Options#CONNECTION_TIMEOUT
     *                         CONNECTION_TIMEOUT}: Timeout in seconds for
     *                         connecting to this storage provider
     *                     <li>{@link Options#WAIT_TIMEOUT WAIT_TIMEOUT}:
     *                         Timeout in seconds for reading from this storage
     *                         provider
     *                     <li>{@link Options#CREDENTIAL CREDENTIAL}: Name of
     *                         the Credential object to be used in data source
     *                     <li>{@link Options#S3_BUCKET_NAME S3_BUCKET_NAME}:
     *                         Name of the Amazon S3 bucket to use as the data
     *                         source
     *                     <li>{@link Options#S3_REGION S3_REGION}: Name of the
     *                         Amazon S3 region where the given bucket is
     *                         located
     *                     <li>{@link Options#S3_USE_VIRTUAL_ADDRESSING
     *                         S3_USE_VIRTUAL_ADDRESSING}: When true (default),
     *                         the requests URI should be specified in
     *                         virtual-hosted-style format where the bucket
     *                         name is part of the domain name in the URL.
     *                         Otherwise set to false to use path-style URI for
     *                         requests.
     *                         Supported values:
     *                         <ul>
     *                             <li>{@link Options#TRUE TRUE}
     *                             <li>{@link Options#FALSE FALSE}
     *                         </ul>
     *                         The default value is {@link Options#TRUE TRUE}.
     *                     <li>{@link Options#S3_AWS_ROLE_ARN S3_AWS_ROLE_ARN}:
     *                         Amazon IAM Role ARN which has required S3
     *                         permissions that can be assumed for the given S3
     *                         IAM user
     *                     <li>{@link Options#S3_ENCRYPTION_CUSTOMER_ALGORITHM
     *                         S3_ENCRYPTION_CUSTOMER_ALGORITHM}: Customer
     *                         encryption algorithm used encrypting data
     *                     <li>{@link Options#S3_ENCRYPTION_CUSTOMER_KEY
     *                         S3_ENCRYPTION_CUSTOMER_KEY}: Customer encryption
     *                         key to encrypt or decrypt data
     *                     <li>{@link Options#HDFS_KERBEROS_KEYTAB
     *                         HDFS_KERBEROS_KEYTAB}: Kerberos keytab file
     *                         location for the given HDFS user.  This may be a
     *                         KIFS file.
     *                     <li>{@link Options#HDFS_DELEGATION_TOKEN
     *                         HDFS_DELEGATION_TOKEN}: Delegation token for the
     *                         given HDFS user
     *                     <li>{@link Options#HDFS_USE_KERBEROS
     *                         HDFS_USE_KERBEROS}: Use kerberos authentication
     *                         for the given HDFS cluster.
     *                         Supported values:
     *                         <ul>
     *                             <li>{@link Options#TRUE TRUE}
     *                             <li>{@link Options#FALSE FALSE}
     *                         </ul>
     *                         The default value is {@link Options#FALSE
     *                         FALSE}.
     *                     <li>{@link Options#AZURE_STORAGE_ACCOUNT_NAME
     *                         AZURE_STORAGE_ACCOUNT_NAME}: Name of the Azure
     *                         storage account to use as the data source, this
     *                         is valid only if tenant_id is specified
     *                     <li>{@link Options#AZURE_CONTAINER_NAME
     *                         AZURE_CONTAINER_NAME}: Name of the Azure storage
     *                         container to use as the data source
     *                     <li>{@link Options#AZURE_TENANT_ID AZURE_TENANT_ID}:
     *                         Active Directory tenant ID (or directory ID)
     *                     <li>{@link Options#AZURE_SAS_TOKEN AZURE_SAS_TOKEN}:
     *                         Shared access signature token for Azure storage
     *                         account to use as the data source
     *                     <li>{@link Options#AZURE_OAUTH_TOKEN
     *                         AZURE_OAUTH_TOKEN}: Oauth token to access given
     *                         storage container
     *                     <li>{@link Options#GCS_BUCKET_NAME GCS_BUCKET_NAME}:
     *                         Name of the Google Cloud Storage bucket to use
     *                         as the data source
     *                     <li>{@link Options#GCS_PROJECT_ID GCS_PROJECT_ID}:
     *                         Name of the Google Cloud project to use as the
     *                         data source
     *                     <li>{@link Options#GCS_SERVICE_ACCOUNT_KEYS
     *                         GCS_SERVICE_ACCOUNT_KEYS}: Google Cloud service
     *                         account keys to use for authenticating the data
     *                         source
     *                     <li>{@link Options#IS_STREAM IS_STREAM}: To load
     *                         from Azure/GCS/S3 as a stream continuously.
     *                         Supported values:
     *                         <ul>
     *                             <li>{@link Options#TRUE TRUE}
     *                             <li>{@link Options#FALSE FALSE}
     *                         </ul>
     *                         The default value is {@link Options#FALSE
     *                         FALSE}.
     *                     <li>{@link Options#KAFKA_TOPIC_NAME
     *                         KAFKA_TOPIC_NAME}: Name of the Kafka topic to
     *                         use as the data source
     *                     <li>{@link Options#JDBC_DRIVER_JAR_PATH
     *                         JDBC_DRIVER_JAR_PATH}: JDBC driver jar file
     *                         location.  This may be a KIFS file.
     *                     <li>{@link Options#JDBC_DRIVER_CLASS_NAME
     *                         JDBC_DRIVER_CLASS_NAME}: Name of the JDBC driver
     *                         class
     *                     <li>{@link Options#ANONYMOUS ANONYMOUS}: Use
     *                         anonymous connection to storage
     *                         provider--DEPRECATED: this is now the default.
     *                         Specify use_managed_credentials for
     *                         non-anonymous connection.
     *                         Supported values:
     *                         <ul>
     *                             <li>{@link Options#TRUE TRUE}
     *                             <li>{@link Options#FALSE FALSE}
     *                         </ul>
     *                         The default value is {@link Options#TRUE TRUE}.
     *                     <li>{@link Options#USE_MANAGED_CREDENTIALS
     *                         USE_MANAGED_CREDENTIALS}: When no credentials
     *                         are supplied, we use anonymous access by
     *                         default.  If this is set, we will use cloud
     *                         provider user settings.
     *                         Supported values:
     *                         <ul>
     *                             <li>{@link Options#TRUE TRUE}
     *                             <li>{@link Options#FALSE FALSE}
     *                         </ul>
     *                         The default value is {@link Options#FALSE
     *                         FALSE}.
     *                     <li>{@link Options#USE_HTTPS USE_HTTPS}: Use https
     *                         to connect to datasource if true, otherwise use
     *                         http.
     *                         Supported values:
     *                         <ul>
     *                             <li>{@link Options#TRUE TRUE}
     *                             <li>{@link Options#FALSE FALSE}
     *                         </ul>
     *                         The default value is {@link Options#TRUE TRUE}.
     *                     <li>{@link Options#SCHEMA_REGISTRY_LOCATION
     *                         SCHEMA_REGISTRY_LOCATION}: Location of Confluent
     *                         Schema registry in
     *                         '[storage_path[:storage_port]]' format.
     *                     <li>{@link Options#SCHEMA_REGISTRY_CREDENTIAL
     *                         SCHEMA_REGISTRY_CREDENTIAL}: Confluent Schema
     *                         registry Credential object name.
     *                     <li>{@link Options#SCHEMA_REGISTRY_PORT
     *                         SCHEMA_REGISTRY_PORT}: Confluent Schema registry
     *                         port (optional).
     *                 </ul>
     *                 The default value is an empty {@link Map}.
     */
    public CreateDatasourceRequest(String name, String location, String userName, String password, Map<String, String> options) {
        this.name = (name == null) ? "" : name;
        this.location = (location == null) ? "" : location;
        this.userName = (userName == null) ? "" : userName;
        this.password = (password == null) ? "" : password;
        this.options = (options == null) ? new LinkedHashMap<String, String>() : options;
    }

    /**
     * Name of the data source to be created.
     *
     * @return The current value of {@code name}.
     */
    public String getName() {
        return name;
    }

    /**
     * Name of the data source to be created.
     *
     * @param name  The new value for {@code name}.
     *
     * @return {@code this} to mimic the builder pattern.
     */
    public CreateDatasourceRequest setName(String name) {
        this.name = (name == null) ? "" : name;
        return this;
    }

    /**
     * Location of the remote storage in
     * 'storage_provider_type://[storage_path[:storage_port]]' format.
     * <p>
     * Supported storage provider types are
     * 'azure','gcs','hdfs','jdbc','kafka', 'confluent' and 's3'.
     *
     * @return The current value of {@code location}.
     */
    public String getLocation() {
        return location;
    }

    /**
     * Location of the remote storage in
     * 'storage_provider_type://[storage_path[:storage_port]]' format.
     * <p>
     * Supported storage provider types are
     * 'azure','gcs','hdfs','jdbc','kafka', 'confluent' and 's3'.
     *
     * @param location  The new value for {@code location}.
     *
     * @return {@code this} to mimic the builder pattern.
     */
    public CreateDatasourceRequest setLocation(String location) {
        this.location = (location == null) ? "" : location;
        return this;
    }

    /**
     * Name of the remote system user; may be an empty string
     *
     * @return The current value of {@code userName}.
     */
    public String getUserName() {
        return userName;
    }

    /**
     * Name of the remote system user; may be an empty string
     *
     * @param userName  The new value for {@code userName}.
     *
     * @return {@code this} to mimic the builder pattern.
     */
    public CreateDatasourceRequest setUserName(String userName) {
        this.userName = (userName == null) ? "" : userName;
        return this;
    }

    /**
     * Password for the remote system user; may be an empty string
     *
     * @return The current value of {@code password}.
     */
    public String getPassword() {
        return password;
    }

    /**
     * Password for the remote system user; may be an empty string
     *
     * @param password  The new value for {@code password}.
     *
     * @return {@code this} to mimic the builder pattern.
     */
    public CreateDatasourceRequest setPassword(String password) {
        this.password = (password == null) ? "" : password;
        return this;
    }

    /**
     * Optional parameters.
     * <ul>
     *     <li>{@link Options#SKIP_VALIDATION SKIP_VALIDATION}: Bypass
     *         validation of connection to remote source.
     *         Supported values:
     *         <ul>
     *             <li>{@link Options#TRUE TRUE}
     *             <li>{@link Options#FALSE FALSE}
     *         </ul>
     *         The default value is {@link Options#FALSE FALSE}.
     *     <li>{@link Options#CONNECTION_TIMEOUT CONNECTION_TIMEOUT}: Timeout
     *         in seconds for connecting to this storage provider
     *     <li>{@link Options#WAIT_TIMEOUT WAIT_TIMEOUT}: Timeout in seconds
     *         for reading from this storage provider
     *     <li>{@link Options#CREDENTIAL CREDENTIAL}: Name of the Credential
     *         object to be used in data source
     *     <li>{@link Options#S3_BUCKET_NAME S3_BUCKET_NAME}: Name of the
     *         Amazon S3 bucket to use as the data source
     *     <li>{@link Options#S3_REGION S3_REGION}: Name of the Amazon S3
     *         region where the given bucket is located
     *     <li>{@link Options#S3_USE_VIRTUAL_ADDRESSING
     *         S3_USE_VIRTUAL_ADDRESSING}: When true (default), the requests
     *         URI should be specified in virtual-hosted-style format where the
     *         bucket name is part of the domain name in the URL.   Otherwise
     *         set to false to use path-style URI for requests.
     *         Supported values:
     *         <ul>
     *             <li>{@link Options#TRUE TRUE}
     *             <li>{@link Options#FALSE FALSE}
     *         </ul>
     *         The default value is {@link Options#TRUE TRUE}.
     *     <li>{@link Options#S3_AWS_ROLE_ARN S3_AWS_ROLE_ARN}: Amazon IAM Role
     *         ARN which has required S3 permissions that can be assumed for
     *         the given S3 IAM user
     *     <li>{@link Options#S3_ENCRYPTION_CUSTOMER_ALGORITHM
     *         S3_ENCRYPTION_CUSTOMER_ALGORITHM}: Customer encryption algorithm
     *         used encrypting data
     *     <li>{@link Options#S3_ENCRYPTION_CUSTOMER_KEY
     *         S3_ENCRYPTION_CUSTOMER_KEY}: Customer encryption key to encrypt
     *         or decrypt data
     *     <li>{@link Options#HDFS_KERBEROS_KEYTAB HDFS_KERBEROS_KEYTAB}:
     *         Kerberos keytab file location for the given HDFS user.  This may
     *         be a KIFS file.
     *     <li>{@link Options#HDFS_DELEGATION_TOKEN HDFS_DELEGATION_TOKEN}:
     *         Delegation token for the given HDFS user
     *     <li>{@link Options#HDFS_USE_KERBEROS HDFS_USE_KERBEROS}: Use
     *         kerberos authentication for the given HDFS cluster.
     *         Supported values:
     *         <ul>
     *             <li>{@link Options#TRUE TRUE}
     *             <li>{@link Options#FALSE FALSE}
     *         </ul>
     *         The default value is {@link Options#FALSE FALSE}.
     *     <li>{@link Options#AZURE_STORAGE_ACCOUNT_NAME
     *         AZURE_STORAGE_ACCOUNT_NAME}: Name of the Azure storage account
     *         to use as the data source, this is valid only if tenant_id is
     *         specified
     *     <li>{@link Options#AZURE_CONTAINER_NAME AZURE_CONTAINER_NAME}: Name
     *         of the Azure storage container to use as the data source
     *     <li>{@link Options#AZURE_TENANT_ID AZURE_TENANT_ID}: Active
     *         Directory tenant ID (or directory ID)
     *     <li>{@link Options#AZURE_SAS_TOKEN AZURE_SAS_TOKEN}: Shared access
     *         signature token for Azure storage account to use as the data
     *         source
     *     <li>{@link Options#AZURE_OAUTH_TOKEN AZURE_OAUTH_TOKEN}: Oauth token
     *         to access given storage container
     *     <li>{@link Options#GCS_BUCKET_NAME GCS_BUCKET_NAME}: Name of the
     *         Google Cloud Storage bucket to use as the data source
     *     <li>{@link Options#GCS_PROJECT_ID GCS_PROJECT_ID}: Name of the
     *         Google Cloud project to use as the data source
     *     <li>{@link Options#GCS_SERVICE_ACCOUNT_KEYS
     *         GCS_SERVICE_ACCOUNT_KEYS}: Google Cloud service account keys to
     *         use for authenticating the data source
     *     <li>{@link Options#IS_STREAM IS_STREAM}: To load from Azure/GCS/S3
     *         as a stream continuously.
     *         Supported values:
     *         <ul>
     *             <li>{@link Options#TRUE TRUE}
     *             <li>{@link Options#FALSE FALSE}
     *         </ul>
     *         The default value is {@link Options#FALSE FALSE}.
     *     <li>{@link Options#KAFKA_TOPIC_NAME KAFKA_TOPIC_NAME}: Name of the
     *         Kafka topic to use as the data source
     *     <li>{@link Options#JDBC_DRIVER_JAR_PATH JDBC_DRIVER_JAR_PATH}: JDBC
     *         driver jar file location.  This may be a KIFS file.
     *     <li>{@link Options#JDBC_DRIVER_CLASS_NAME JDBC_DRIVER_CLASS_NAME}:
     *         Name of the JDBC driver class
     *     <li>{@link Options#ANONYMOUS ANONYMOUS}: Use anonymous connection to
     *         storage provider--DEPRECATED: this is now the default.  Specify
     *         use_managed_credentials for non-anonymous connection.
     *         Supported values:
     *         <ul>
     *             <li>{@link Options#TRUE TRUE}
     *             <li>{@link Options#FALSE FALSE}
     *         </ul>
     *         The default value is {@link Options#TRUE TRUE}.
     *     <li>{@link Options#USE_MANAGED_CREDENTIALS USE_MANAGED_CREDENTIALS}:
     *         When no credentials are supplied, we use anonymous access by
     *         default.  If this is set, we will use cloud provider user
     *         settings.
     *         Supported values:
     *         <ul>
     *             <li>{@link Options#TRUE TRUE}
     *             <li>{@link Options#FALSE FALSE}
     *         </ul>
     *         The default value is {@link Options#FALSE FALSE}.
     *     <li>{@link Options#USE_HTTPS USE_HTTPS}: Use https to connect to
     *         datasource if true, otherwise use http.
     *         Supported values:
     *         <ul>
     *             <li>{@link Options#TRUE TRUE}
     *             <li>{@link Options#FALSE FALSE}
     *         </ul>
     *         The default value is {@link Options#TRUE TRUE}.
     *     <li>{@link Options#SCHEMA_REGISTRY_LOCATION
     *         SCHEMA_REGISTRY_LOCATION}: Location of Confluent Schema registry
     *         in '[storage_path[:storage_port]]' format.
     *     <li>{@link Options#SCHEMA_REGISTRY_CREDENTIAL
     *         SCHEMA_REGISTRY_CREDENTIAL}: Confluent Schema registry
     *         Credential object name.
     *     <li>{@link Options#SCHEMA_REGISTRY_PORT SCHEMA_REGISTRY_PORT}:
     *         Confluent Schema registry port (optional).
     * </ul>
     * The default value is an empty {@link Map}.
     *
     * @return The current value of {@code options}.
     */
    public Map<String, String> getOptions() {
        return options;
    }

    /**
     * Optional parameters.
     * <ul>
     *     <li>{@link Options#SKIP_VALIDATION SKIP_VALIDATION}: Bypass
     *         validation of connection to remote source.
     *         Supported values:
     *         <ul>
     *             <li>{@link Options#TRUE TRUE}
     *             <li>{@link Options#FALSE FALSE}
     *         </ul>
     *         The default value is {@link Options#FALSE FALSE}.
     *     <li>{@link Options#CONNECTION_TIMEOUT CONNECTION_TIMEOUT}: Timeout
     *         in seconds for connecting to this storage provider
     *     <li>{@link Options#WAIT_TIMEOUT WAIT_TIMEOUT}: Timeout in seconds
     *         for reading from this storage provider
     *     <li>{@link Options#CREDENTIAL CREDENTIAL}: Name of the Credential
     *         object to be used in data source
     *     <li>{@link Options#S3_BUCKET_NAME S3_BUCKET_NAME}: Name of the
     *         Amazon S3 bucket to use as the data source
     *     <li>{@link Options#S3_REGION S3_REGION}: Name of the Amazon S3
     *         region where the given bucket is located
     *     <li>{@link Options#S3_USE_VIRTUAL_ADDRESSING
     *         S3_USE_VIRTUAL_ADDRESSING}: When true (default), the requests
     *         URI should be specified in virtual-hosted-style format where the
     *         bucket name is part of the domain name in the URL.   Otherwise
     *         set to false to use path-style URI for requests.
     *         Supported values:
     *         <ul>
     *             <li>{@link Options#TRUE TRUE}
     *             <li>{@link Options#FALSE FALSE}
     *         </ul>
     *         The default value is {@link Options#TRUE TRUE}.
     *     <li>{@link Options#S3_AWS_ROLE_ARN S3_AWS_ROLE_ARN}: Amazon IAM Role
     *         ARN which has required S3 permissions that can be assumed for
     *         the given S3 IAM user
     *     <li>{@link Options#S3_ENCRYPTION_CUSTOMER_ALGORITHM
     *         S3_ENCRYPTION_CUSTOMER_ALGORITHM}: Customer encryption algorithm
     *         used encrypting data
     *     <li>{@link Options#S3_ENCRYPTION_CUSTOMER_KEY
     *         S3_ENCRYPTION_CUSTOMER_KEY}: Customer encryption key to encrypt
     *         or decrypt data
     *     <li>{@link Options#HDFS_KERBEROS_KEYTAB HDFS_KERBEROS_KEYTAB}:
     *         Kerberos keytab file location for the given HDFS user.  This may
     *         be a KIFS file.
     *     <li>{@link Options#HDFS_DELEGATION_TOKEN HDFS_DELEGATION_TOKEN}:
     *         Delegation token for the given HDFS user
     *     <li>{@link Options#HDFS_USE_KERBEROS HDFS_USE_KERBEROS}: Use
     *         kerberos authentication for the given HDFS cluster.
     *         Supported values:
     *         <ul>
     *             <li>{@link Options#TRUE TRUE}
     *             <li>{@link Options#FALSE FALSE}
     *         </ul>
     *         The default value is {@link Options#FALSE FALSE}.
     *     <li>{@link Options#AZURE_STORAGE_ACCOUNT_NAME
     *         AZURE_STORAGE_ACCOUNT_NAME}: Name of the Azure storage account
     *         to use as the data source, this is valid only if tenant_id is
     *         specified
     *     <li>{@link Options#AZURE_CONTAINER_NAME AZURE_CONTAINER_NAME}: Name
     *         of the Azure storage container to use as the data source
     *     <li>{@link Options#AZURE_TENANT_ID AZURE_TENANT_ID}: Active
     *         Directory tenant ID (or directory ID)
     *     <li>{@link Options#AZURE_SAS_TOKEN AZURE_SAS_TOKEN}: Shared access
     *         signature token for Azure storage account to use as the data
     *         source
     *     <li>{@link Options#AZURE_OAUTH_TOKEN AZURE_OAUTH_TOKEN}: Oauth token
     *         to access given storage container
     *     <li>{@link Options#GCS_BUCKET_NAME GCS_BUCKET_NAME}: Name of the
     *         Google Cloud Storage bucket to use as the data source
     *     <li>{@link Options#GCS_PROJECT_ID GCS_PROJECT_ID}: Name of the
     *         Google Cloud project to use as the data source
     *     <li>{@link Options#GCS_SERVICE_ACCOUNT_KEYS
     *         GCS_SERVICE_ACCOUNT_KEYS}: Google Cloud service account keys to
     *         use for authenticating the data source
     *     <li>{@link Options#IS_STREAM IS_STREAM}: To load from Azure/GCS/S3
     *         as a stream continuously.
     *         Supported values:
     *         <ul>
     *             <li>{@link Options#TRUE TRUE}
     *             <li>{@link Options#FALSE FALSE}
     *         </ul>
     *         The default value is {@link Options#FALSE FALSE}.
     *     <li>{@link Options#KAFKA_TOPIC_NAME KAFKA_TOPIC_NAME}: Name of the
     *         Kafka topic to use as the data source
     *     <li>{@link Options#JDBC_DRIVER_JAR_PATH JDBC_DRIVER_JAR_PATH}: JDBC
     *         driver jar file location.  This may be a KIFS file.
     *     <li>{@link Options#JDBC_DRIVER_CLASS_NAME JDBC_DRIVER_CLASS_NAME}:
     *         Name of the JDBC driver class
     *     <li>{@link Options#ANONYMOUS ANONYMOUS}: Use anonymous connection to
     *         storage provider--DEPRECATED: this is now the default.  Specify
     *         use_managed_credentials for non-anonymous connection.
     *         Supported values:
     *         <ul>
     *             <li>{@link Options#TRUE TRUE}
     *             <li>{@link Options#FALSE FALSE}
     *         </ul>
     *         The default value is {@link Options#TRUE TRUE}.
     *     <li>{@link Options#USE_MANAGED_CREDENTIALS USE_MANAGED_CREDENTIALS}:
     *         When no credentials are supplied, we use anonymous access by
     *         default.  If this is set, we will use cloud provider user
     *         settings.
     *         Supported values:
     *         <ul>
     *             <li>{@link Options#TRUE TRUE}
     *             <li>{@link Options#FALSE FALSE}
     *         </ul>
     *         The default value is {@link Options#FALSE FALSE}.
     *     <li>{@link Options#USE_HTTPS USE_HTTPS}: Use https to connect to
     *         datasource if true, otherwise use http.
     *         Supported values:
     *         <ul>
     *             <li>{@link Options#TRUE TRUE}
     *             <li>{@link Options#FALSE FALSE}
     *         </ul>
     *         The default value is {@link Options#TRUE TRUE}.
     *     <li>{@link Options#SCHEMA_REGISTRY_LOCATION
     *         SCHEMA_REGISTRY_LOCATION}: Location of Confluent Schema registry
     *         in '[storage_path[:storage_port]]' format.
     *     <li>{@link Options#SCHEMA_REGISTRY_CREDENTIAL
     *         SCHEMA_REGISTRY_CREDENTIAL}: Confluent Schema registry
     *         Credential object name.
     *     <li>{@link Options#SCHEMA_REGISTRY_PORT SCHEMA_REGISTRY_PORT}:
     *         Confluent Schema registry port (optional).
     * </ul>
     * The default value is an empty {@link Map}.
     *
     * @param options  The new value for {@code options}.
     *
     * @return {@code this} to mimic the builder pattern.
     */
    public CreateDatasourceRequest setOptions(Map<String, String> options) {
        this.options = (options == null) ? new LinkedHashMap<String, String>() : options;
        return this;
    }

    /**
     * This method supports the Avro framework and is not intended to be called
     * directly by the user.
     *
     * @return The schema object describing this class.
     */
    @Override
    public Schema getSchema() {
        return schema$;
    }

    /**
     * This method supports the Avro framework and is not intended to be called
     * directly by the user.
     *
     * @param index  the position of the field to get
     *
     * @return value of the field with the given index.
     *
     * @throws IndexOutOfBoundsException
     */
    @Override
    public Object get(int index) {
        switch (index) {
            case 0:
                return this.name;

            case 1:
                return this.location;

            case 2:
                return this.userName;

            case 3:
                return this.password;

            case 4:
                return this.options;

            default:
                throw new IndexOutOfBoundsException("Invalid index specified.");
        }
    }

    /**
     * This method supports the Avro framework and is not intended to be called
     * directly by the user.
     *
     * @param index  the position of the field to set
     * @param value  the value to set
     *
     * @throws IndexOutOfBoundsException
     */
    @Override
    @SuppressWarnings("unchecked")
    public void put(int index, Object value) {
        switch (index) {
            case 0:
                this.name = (String)value;
                break;

            case 1:
                this.location = (String)value;
                break;

            case 2:
                this.userName = (String)value;
                break;

            case 3:
                this.password = (String)value;
                break;

            case 4:
                this.options = (Map<String, String>)value;
                break;

            default:
                throw new IndexOutOfBoundsException("Invalid index specified.");
        }
    }

    @Override
    public boolean equals(Object obj) {
        if( obj == this ) {
            return true;
        }

        if( (obj == null) || (obj.getClass() != this.getClass()) ) {
            return false;
        }

        CreateDatasourceRequest that = (CreateDatasourceRequest)obj;

        return ( this.name.equals( that.name )
                 && this.location.equals( that.location )
                 && this.userName.equals( that.userName )
                 && this.password.equals( that.password )
                 && this.options.equals( that.options ) );
    }

    @Override
    public String toString() {
        GenericData gd = GenericData.get();
        StringBuilder builder = new StringBuilder();
        builder.append( "{" );
        builder.append( gd.toString( "name" ) );
        builder.append( ": " );
        builder.append( gd.toString( this.name ) );
        builder.append( ", " );
        builder.append( gd.toString( "location" ) );
        builder.append( ": " );
        builder.append( gd.toString( this.location ) );
        builder.append( ", " );
        builder.append( gd.toString( "userName" ) );
        builder.append( ": " );
        builder.append( gd.toString( this.userName ) );
        builder.append( ", " );
        builder.append( gd.toString( "password" ) );
        builder.append( ": " );
        builder.append( gd.toString( this.password ) );
        builder.append( ", " );
        builder.append( gd.toString( "options" ) );
        builder.append( ": " );
        builder.append( gd.toString( this.options ) );
        builder.append( "}" );

        return builder.toString();
    }

    @Override
    public int hashCode() {
        int hashCode = 1;
        hashCode = (31 * hashCode) + this.name.hashCode();
        hashCode = (31 * hashCode) + this.location.hashCode();
        hashCode = (31 * hashCode) + this.userName.hashCode();
        hashCode = (31 * hashCode) + this.password.hashCode();
        hashCode = (31 * hashCode) + this.options.hashCode();
        return hashCode;
    }
}
